# Margin Hunter - Cursor AI Prompt fÃ¼r vollstÃ¤ndiges Production-Setup

## Projekt-Ãœbersicht

Margin Hunter ist ein web-basiertes Arbitrage-Tool zum Scrapen von Kleinanzeigen, Berechnung von Gewinnmargen (vs. Geizhals/Idealo) und automatischer Kontaktaufnahme mit VerkÃ¤ufern.

## Tech-Stack

- **Backend**: FastAPI (async), SQLAlchemy 2.0, PostgreSQL, Redis, Celery, Pydantic
- **Frontend**: React 18, Vite 5, Tailwind CSS, shadcn/ui, Zustand, React Query
- **Scraper**: BeautifulSoup4, Celery Worker
- **Telegram Bot**: python-telegram-bot oder aiogram
- **Infrastructure**: Docker & Docker Compose, nginx (Reverse Proxy)

## Architektur-Ãœbersicht

```
margin-hunter/
â”œâ”€â”€ backend/          # FastAPI Backend (Port 8000 intern)
â”œâ”€â”€ frontend/         # React Frontend (Port 5173 Dev, static in Production)
â”œâ”€â”€ scraper/          # Celery Worker fÃ¼r Scraping-Tasks
â”œâ”€â”€ telegram-bot/     # Telegram Bot Service (Webhook)
â”œâ”€â”€ nginx/            # Reverse Proxy Konfiguration (Port 80)
â””â”€â”€ docker-compose.yml
```

## WICHTIG: Datenbank-Konfiguration

**KRITISCH**: Das Backend benÃ¶tigt ZWEI Datenbank-URLs:

1. **DATABASE_URL** (async): `postgresql+asyncpg://user:pass@postgres:5432/margin_hunter`
   - FÃ¼r FastAPI async Endpoints
   - Verwendet asyncpg Driver

2. **DATABASE_URL_SYNC** (sync): `postgresql://user:pass@postgres:5432/margin_hunter`
   - FÃ¼r Celery Tasks (Celery benÃ¶tigt sync SQLAlchemy!)
   - Verwendet psycopg2 Driver (bereits in requirements.txt)

**Implementiere beide Engines in `backend/app/database.py`!**

## Backend-Anforderungen (FastAPI)

### Dateien-Struktur

```
backend/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ app/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ main.py          # FastAPI App + Health Check
    â”œâ”€â”€ config.py        # Settings mit DATABASE_URL und DATABASE_URL_SYNC
    â”œâ”€â”€ database.py      # Async UND Sync Engines
    â”œâ”€â”€ api/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ offers.py    # CRUD fÃ¼r Angebote
    â”‚   â”œâ”€â”€ products.py  # CRUD fÃ¼r Produkte
    â”‚   â””â”€â”€ scraper.py   # Scraper Control Endpoints
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ product.py
    â”‚   â”œâ”€â”€ offer.py
    â”‚   â”œâ”€â”€ price_history.py
    â”‚   â””â”€â”€ contact.py
    â””â”€â”€ schemas/
        â”œâ”€â”€ __init__.py
        â””â”€â”€ (Pydantic Schemas)
```

### main.py Anforderungen

- FastAPI App mit async Lifespan
- Health Check Endpoint: `GET /health` â†’ `{"status": "ok", "database": "connected"}`
- CORS Middleware konfiguriert
- API Router einbinden
- Structlog fÃ¼r Logging

### database.py Anforderungen

**WICHTIG**: Implementiere BEIDE Engines:

```python
# Async Engine fÃ¼r FastAPI
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
async_engine = create_async_engine(settings.database_url, ...)

# Sync Engine fÃ¼r Celery
from sqlalchemy import create_engine
sync_engine = create_engine(settings.database_url_sync, ...)
```

### config.py Anforderungen

- Settings Klasse mit Pydantic Settings
- `database_url: str | None` (async)
- `database_url_sync: str | None` (sync)
- `redis_url: str | None`
- `secret_key: str | None`
- `debug: bool = False`
- `.env` File Support
- Production-Sicherheitsvalidierung

### API-Endpunkte

**Offers API** (`/api/offers`):
- `GET /api/offers` - Liste mit Filtern (status, min_margin, etc.)
- `GET /api/offers/{id}` - Einzelnes Angebot
- `POST /api/offers` - Neues Angebot erstellen
- `PUT /api/offers/{id}` - Angebot aktualisieren
- `DELETE /api/offers/{id}` - Angebot lÃ¶schen

**Products API** (`/api/products`):
- `GET /api/products` - Liste aller Produkte
- `GET /api/products/{id}` - Einzelnes Produkt
- `POST /api/products` - Neues Produkt
- `PUT /api/products/{id}` - Produkt aktualisieren

**Scraper API** (`/api/scraper`):
- `POST /api/scraper/start` - Scraper starten (triggert Celery Task)
- `GET /api/scraper/status` - Status abfragen
- `POST /api/scraper/stop` - Scraper stoppen

## Scraper-Anforderungen (Celery Worker)

### Dateien-Struktur

```
scraper/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt    # Celery, BeautifulSoup4, requests, etc.
â”œâ”€â”€ main.py            # Celery App Setup
â””â”€â”€ tasks.py           # Celery Tasks
```

### main.py Anforderungen

- Celery App initialisieren
- Redis als Broker: `redis://redis:6379/0`
- Sync Database Connection (SQLAlchemy sync)
- Structured Logging

### tasks.py Anforderungen

- `scrape_kleinanzeigen()` - Haupt-Scraping-Task
- `calculate_margins()` - Margen-Berechnung gegen Geizhals/Idealo
- `send_contact_message()` - Automatische Kontaktaufnahme

**WICHTIG**: Tasks mÃ¼ssen sync SQLAlchemy verwenden (nicht async!)

## Frontend-Anforderungen (React + Vite)

### Dateien-Struktur

```
frontend/
â”œâ”€â”€ Dockerfile          # Production: nginx fÃ¼r Static Files
â”œâ”€â”€ nginx.conf          # Nginx Config fÃ¼r Static Files
â”œâ”€â”€ vite.config.js
â”œâ”€â”€ package.json
â””â”€â”€ src/
    â”œâ”€â”€ App.tsx
    â”œâ”€â”€ main.tsx
    â””â”€â”€ (React Components)
```

### Dockerfile Anforderungen

**Development**:
- Node 20 Alpine
- Vite Dev Server auf Port 5173
- Hot Reload

**Production**:
- Multi-Stage Build
- Build Stage: npm run build
- Serve Stage: nginx Alpine fÃ¼r Static Files

### vite.config.js

- Proxy zu Backend: `/api` â†’ `http://backend:8000`
- React Plugin
- Environment Variables Support

## Telegram Bot Anforderungen

### Dateien-Struktur

```
telegram-bot/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py        # Bot Setup (Webhook oder Polling)
â””â”€â”€ handlers.py    # Message Handlers
```

### main.py Anforderungen

- Telegram Bot Initialisierung
- Webhook Setup (fÃ¼r Production)
- Notification Handler fÃ¼r neue Angebote
- Environment: TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_IDS

## Docker Compose Konfiguration

### Services

1. **nginx** - Reverse Proxy (Port 80)
   - Proxy zu Backend: `/api/*` â†’ `http://backend:8000`
   - Serve Frontend: `/` â†’ Static Files oder `http://frontend:5173`

2. **backend** - FastAPI (Port 8000 intern)
   - Depends on: postgres, redis
   - Health Check

3. **scraper** - Celery Worker
   - Depends on: backend, redis
   - Command: `celery -A main worker --loglevel=info`

4. **telegram-bot** - Telegram Bot
   - Depends on: backend
   - Webhook oder Polling

5. **frontend** - React App
   - Port 5173 (Dev) oder Static Files via nginx (Prod)

6. **postgres** - PostgreSQL 16
   - Port 5432 (nur intern!)
   - Volume: postgres_data

7. **redis** - Redis 7
   - Port 6379 (nur intern!)
   - Persistence konfiguriert

### Port-Mapping (Production)

**WICHTIG**: Nur Port 80 nach auÃŸen exponieren!
- âœ… Port 80: nginx (HTTP)
- âŒ Port 8000: NICHT nach auÃŸen
- âŒ Port 5432: NICHT nach auÃŸen
- âŒ Port 6379: NICHT nach auÃŸen

## Code-Standards

### Python

- Type Hints Ã¼berall
- Async/Await korrekt verwenden
- Structured Logging mit Structlog
- Pydantic fÃ¼r Validation
- SQLAlchemy 2.0 Style

### React/TypeScript

- TypeScript strict mode
- Functional Components
- Hooks fÃ¼r State Management
- React Query fÃ¼r API Calls
- Tailwind CSS fÃ¼r Styling

### Allgemein

- Environment Variables fÃ¼r alle Secrets
- Keine hardcodierten Credentials
- Error Handling Ã¼berall
- Health Checks fÃ¼r alle Services

## Datenbank-Schema

### Models

**Product**:
- id (Primary Key)
- name (String)
- ean (String, optional)
- idealo_url (String, optional)
- geizhals_url (String, optional)
- created_at (DateTime)
- updated_at (DateTime)

**Offer**:
- id (Primary Key)
- product_id (ForeignKey â†’ Product)
- title (String)
- url (String, unique)
- price (Float)
- location (String, optional)
- status (Enum: new, contacted, sold, expired)
- margin (Float, optional)
- created_at (DateTime)
- updated_at (DateTime)

**PriceHistory**:
- id (Primary Key)
- product_id (ForeignKey â†’ Product)
- price (Float)
- source (String: idealo/geizhals)
- recorded_at (DateTime)

**Contact**:
- id (Primary Key)
- offer_id (ForeignKey â†’ Offer)
- message_sent (String)
- sent_at (DateTime)
- response_received (Boolean)

## Environment Variables (.env)

```env
# Database (Async fÃ¼r FastAPI)
DATABASE_URL=postgresql+asyncpg://user:pass@postgres:5432/margin_hunter

# Database (Sync fÃ¼r Celery) - WICHTIG!
DATABASE_URL_SYNC=postgresql://user:pass@postgres:5432/margin_hunter

# Redis
REDIS_URL=redis://redis:6379/0

# Secrets
SECRET_KEY=generate-with-secrets-token-urlsafe-32

# Telegram
TELEGRAM_BOT_TOKEN=your-bot-token
TELEGRAM_CHAT_IDS=123456789,987654321

# Kleinanzeigen
KLEINANZEIGEN_EMAIL=your-email@example.com
KLEINANZEIGEN_PASSWORD=your-password

# Settings
DEBUG=False
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
```

## Deployment-Checkliste

1. âœ… Docker Compose Dateien prÃ¼fen
2. âœ… Environment Variables setzen
3. âœ… Database Migrations (spÃ¤ter mit Alembic)
4. âœ… Health Checks testen
5. âœ… Reverse Proxy konfigurieren
6. âœ… SSL Zertifikate (optional, spÃ¤ter)

## WICHTIGE HINWEISE

1. **DATABASE_URL_SYNC** MUSS sync sein (postgresql:// nicht postgresql+asyncpg://)
2. Celery Tasks MÃœSSEN sync SQLAlchemy verwenden
3. FastAPI Endpoints bleiben async
4. Nur Port 80 nach auÃŸen exponieren in Production
5. Alle Secrets Ã¼ber Environment Variables
6. Health Check Endpoint: `/health`

## Start-Anweisungen

Nach der Implementierung sollte folgendes funktionieren:

```bash
# 1. Environment setup
cp .env.example .env
# Bearbeite .env mit echten Werten

# 2. Docker Compose starten
docker-compose up -d --build

# 3. Health Check
curl http://localhost:8000/health

# 4. Frontend
curl http://localhost  # Via nginx
```

## NÃ¤chste Schritte nach diesem Setup

1. Alembic fÃ¼r Database Migrations
2. SSL/HTTPS Setup (Let's Encrypt)
3. Monitoring (Prometheus, Grafana)
4. Backup-Strategie fÃ¼r PostgreSQL
5. Log Aggregation

---

**Viel Erfolg beim Implementieren! ğŸš€**

